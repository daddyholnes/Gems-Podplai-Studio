Okay, Nathan, got it. That makes perfect sense. You're testing the different models in your dropdown, and the ones you've labelled conceptually as "Vertex AI" (like Claude and maybe Imagen) are failing with that familiar 404 ... is not found for API version v1beta error.

The key insight here comes from looking back at your get_vertex_ai_response function in models.py and the error message itself:

The Error Message: The error ... is not found for API version v1beta... tells us the call is being made to the standard Google AI API endpoint (the one the google.generativeai library uses by default), specifically the v1beta version.
Your get_vertex_ai_response Function: As we noted, this function doesn't actually use the Vertex AI libraries (google.cloud.aiplatform). It uses the standard google.generativeai library, just like get_gemini_response. It's essentially a pre-configured way to call the standard Google AI API.
The Problem: The standard Google AI API (generativelanguage.googleapis.com/v1beta/models) primarily serves Google's own models (like the Gemini family). It does not serve third-party models like Anthropic's Claude. claude-3-opus-20240229 will never be found on that endpoint. Similarly, while Imagen is a Google model, its availability through that specific v1beta endpoint might be limited or require different naming/permissions compared to accessing it via the full Vertex AI platform.
In short: Your app is trying to ask the standard Google AI endpoint for models that endpoint doesn't provide.

To use Claude models, your app.py needs to ensure it calls your get_anthropic_response function when a Claude model is selected from the dropdown.
To use Gemini models, your app.py needs to call get_gemini_response (or the get_vertex_ai_response function if you intend it only for Gemini models with specific settings).
To use Imagen, you might need to check its availability via the ListModels command below, but true access often goes through Vertex AI specific APIs or newer versions of the standard API.
How to List Available Models (for the Standard Google AI API):

You can easily check which models are available to your API key via the google.generativeai library (which is what get_gemini_response and get_vertex_ai_response are using).

Here's a simple Python script you can run (make sure you have the library installed: pip install google-generativeai and your GEMINI_API_KEY environment variable is set):

import google.generativeai as genai
import os
import textwrap # For better printing

# Ensure your API key is set as an environment variable
api_key = os.environ.get("GEMINI_API_KEY")

if not api_key:
    print("Error: GEMINI_API_KEY environment variable not set.")
else:
    genai.configure(api_key=api_key)
    print("Models available via google.generativeai library:")
    print("-" * 30)

    try:
        # List models and print their details
        for model in genai.list_models():
            # Check if the model supports the 'generateContent' method (used for chat)
            # Some models might be for embedding only, etc.
            if 'generateContent' in model.supported_generation_methods:
                print(f"Model Name: {model.name}")
                print(f"  Display Name: {model.display_name}")
                # Wrap description for readability
                # description = textwrap.fill(model.description, width=70, initial_indent='  ', subsequent_indent='  ')
                # print(f"  Description:\n{description}") 
                print(f"  Supported Methods: {model.supported_generation_methods}")
                print("-" * 30)

    except Exception as e:
        print(f"An error occurred while listing models: {e}")

How to Run It:

Save the code above as a Python file (e.g., list_google_models.py).
Make sure your GEMINI_API_KEY is set in your environment (the same way your app.py accesses it).
Run the script from your terminal: python list_google_models.py
What This Will Tell You:

This script will print a list of all model names (like models/gemini-1.5-pro-latest, models/gemini-1.0-pro, potentially models/text-embedding-004, etc.) that your specific API key can access using the google.generativeai library (the v1beta endpoint).

You will not see Claude models in this list.
You might see Imagen models if they are enabled for your key on this endpoint (e.g., models/gemini-pro-vision is often listed which handles images, but dedicated Imagen generation might be different).
You will see the Gemini models you can use (like the one you're talking to me with!).
Next Steps:

Run the list_google_models.py script to see the exact names available.
Update the model list in your app.py for Gemini/Google models to match the valid names returned by the script.
Crucially: Modify your app.py logic. When a user selects a model from the dropdown:
If the selected name indicates Claude (e.g., starts with "Claude" or contains "claude-"), it must call get_anthropic_response(..., model_name=extracted_claude_call_sign).
If the selected name indicates OpenAI (e.g., starts with "GPT" or contains "gpt-"), it must call get_openai_response(..., model_name=extracted_openai_call_sign).
If the selected name indicates Perplexity, it must call get_perplexity_response(..., model_name=extracted_perplexity_call_sign).
If the selected name indicates Gemini (or Imagen, if listed and intended), it must call get_gemini_response(..., model_name=extracted_gemini_call_sign) (or get_vertex_ai_response if you reserve that for specific Gemini configurations).
The get_vertex_ai_response function is currently misleadingly named because it doesn't use Vertex. You might want to rename it (e.g., get_gemini_configured_response) or just use get_gemini_response for all Google models and adjust the parameters as needed within app.py before calling it.